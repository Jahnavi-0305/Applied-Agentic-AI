# -*- coding: utf-8 -*-
"""pinecone_rag_pipeline_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tBZtT9XkOYM9itOpwoz_nQ7n1n027vG4

# Pinecone RAG Pipeline Demo

This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) pipeline using Pinecone and LangChain. We'll walk through the process of:

1. Setting up the environment and installing necessary libraries.
2. Initializing Pinecone and creating a serverless vector index.
3. Loading documents from a local directory.
4. Processing and chunking the documents for effective retrieval.
5. Generating embeddings for these chunks using a sentence transformer model.
6. Storing these embeddings in the Pinecone index.
7. Creating a conversational retrieval chain using LangChain and a Large Language Model (LLM) like GPT.
8. Interacting with our documents through a chat interface that leverages the RAG pipeline.

## What is RAG?
Retrieval Augmented Generation (RAG) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) by dynamically providing them with relevant context from an external knowledge base during the generation process. This helps to:
- **Improve Accuracy**: LLM responses are grounded in factual data from your documents, leading to more accurate and reliable answers.
- **Reduce Hallucinations**: By providing specific context, RAG minimizes the chances of the LLM generating plausible but incorrect or nonsensical information.
- **Keep Information Up-to-Date**: The LLM can access the latest information stored in your vector database without needing to be retrained.
- **Provide Source Attribution**: RAG pipelines can often point to the source documents used to generate an answer, allowing for verification and further exploration.
- **Enable Domain-Specific Knowledge**: You can make the LLM an expert on your specific documents or domain without expensive fine-tuning.

## Prerequisites
- A Pinecone account and API key (sign up at [pinecone.io](https://www.pinecone.io/))
- An OpenAI API key (if using OpenAI for chat completion, sign up at [openai.com](https://openai.com/))
- Python 3.8 or newer.
- Documents (e.g., `.txt`, `.md` files) in a local input folder (we'll create `input_docs` for this demo).

## 1. Install Required Libraries

First, let's install the necessary Python packages. We'll need:
- `langchain`, `langchain-community`, `langchain-openai`: Core LangChain libraries for building our RAG pipeline, including document loaders, text splitters, embedding wrappers, vector store integrations, and LLM integrations.
- `pinecone-client`: The official Python client for interacting with your Pinecone vector database (version >= 3.x.x recommended for `ServerlessSpec`).
- `sentence-transformers`: For generating high-quality sentence and text embeddings.
- `python-dotenv`: For managing environment variables securely (like API keys).
- `openai`: The official Python client for OpenAI, used by LangChain for LLM interactions if you choose an OpenAI model.
- `unstructured`: A library for easily ingesting and preprocessing unstructured data from various file types (e.g., TXT, PDF, HTML).

Uncomment the following cell to install these packages if you haven't already.

### upload test documents to "input_docs" to index and ask questions
"""

# Install required packages
!pip install langchain langchain-community langchain-openai pinecone-client sentence-transformers python-dotenv openai unstructured "unstructured[md]" "unstructured[txt]"

"""## 2. Load Environment Variables and Initialize Components

We'll load our API keys from a `.env` file for security and ease of configuration. Then, we initialize the Pinecone client and the embedding model we'll use throughout the notebook.

**Create a `.env` file** in the same directory as this notebook with the following content:
```
PINECONE_API_KEY='YOUR_PINECONE_API_KEY'
OPENAI_API_KEY='YOUR_OPENAI_API_KEY'
```
Replace `'YOUR_PINECONE_API_KEY'` and `'YOUR_OPENAI_API_KEY'` with your actual keys.
"""

import os
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from langchain_community.document_loaders import DirectoryLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI # Updated import
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import Pinecone as LangChainPinecone # Renamed to avoid conflict

# Load environment variables from .env file
load_dotenv()

# Get API keys from environment
PINECONE_API_KEY = "pcsk_5beLwf_PuSzGdoiKMNjurLv7MQEZ1Hp9zfeEDYrrKD1cxZMaNcExSKTNbmTE2RUDoKkv8e"#os.getenv('PINECONE_API_KEY')
OPENAI_API_KEY = "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"#os.getenv('OPENAI_API_KEY')

# Verify API keys are loaded
if not PINECONE_API_KEY:
    raise ValueError("PINECONE_API_KEY not found. Please set it in your .env file or environment.")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found. Please set it in your .env file or environment.")

# Initialize Pinecone client
# This client is used for direct Pinecone operations like creating/deleting indexes
pc = Pinecone(api_key=PINECONE_API_KEY)

# Initialize embedding model
# We use HuggingFaceEmbeddings with a specific sentence transformer model.
# 'all-MiniLM-L6-v2' is a good starting model: fast and effective, producing 384-dimensional embeddings.
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
EMBEDDING_DIMENSION = 384 # This must match the output dimension of EMBEDDING_MODEL_NAME

print('Successfully initialized Pinecone client and embedding model.')

"""## 3. Set Up Pinecone Index

Now we'll initialize Pinecone and create an index if it doesn't already exist. Pinecone will store our document embeddings and enable fast similarity searches, which is crucial for the retrieval step in RAG.

### What is a Pinecone Index?
A Pinecone index is a dedicated, managed, and scalable environment for storing and searching your vector embeddings. Key features:
- **Stores Vector Embeddings**: Efficiently stores the numerical representations of your text data.
- **Fast Similarity Search**: Uses Approximate Nearest Neighbor (ANN) algorithms to quickly find the most similar vectors to a query vector.
- **Scalability**: Can scale to store and search billions of vectors.
- **Real-time Performance**: Provides low-latency search results.
- **Metadata Filtering**: Allows you to filter search results based on metadata associated with your vectors.

We will use a **Serverless Index** for this demo. Serverless indexes in Pinecone automatically scale based on usage, offering a cost-effective solution that separates storage and compute, meaning you primarily pay for what you use.

**Note**: The new Pinecone SDK (version 3.x and later, using `pinecone-client`) simplifies initialization. You no longer need to specify an `environment` as it's handled by the API key.
"""

# Set index parameters
INDEX_NAME = "rag-demo-notebook" # Choose a unique name for your index
# EMBEDDING_DIMENSION is already defined as 384 from the embedding model initialization
METRIC = "cosine" # Cosine similarity is common for semantic search with text embeddings

try:
    # Check if the index already exists
    existing_indexes = pc.list_indexes()
    if INDEX_NAME not in [index.name for index in existing_indexes]:
        print(f"Index '{INDEX_NAME}' does not exist. Creating new index...")
        # Create a new serverless index
        pc.create_index(
            name=INDEX_NAME,
            dimension=EMBEDDING_DIMENSION,
            metric=METRIC,
            spec=ServerlessSpec(
                cloud="aws",         # Specify your preferred cloud provider
                region="us-east-1"   # Specify your preferred region
            )
        )
        print(f"Successfully created new index: '{INDEX_NAME}' with dimension {EMBEDDING_DIMENSION} and metric '{METRIC}'.")
    else:
        print(f"Using existing index: '{INDEX_NAME}'")

    # Connect to the index (this is more for direct operations, LangChain will also connect)
    index = pc.Index(INDEX_NAME)
    print(f"Successfully connected to index '{INDEX_NAME}'.")
    print(f"Index stats: {index.describe_index_stats()}")

except Exception as e:
    print(f"Error creating/connecting to Pinecone index '{INDEX_NAME}': {str(e)}")
    raise  # Re-raise the exception after logging to stop execution if setup fails

"""## 4. Load and Process Documents

In this section, we'll:
1. **Load documents**: We'll use LangChain's `DirectoryLoader` to load all files from a specified input directory (`input_docs`). `UnstructuredFileLoader` will be used under the hood to handle various file formats (like `.txt`, `.md`).
2. **Split documents into chunks**: Large documents often exceed the context window limits of LLMs. More importantly, for RAG, smaller, focused chunks lead to more precise retrieval of relevant information. We'll use `RecursiveCharacterTextSplitter`.

### Why Chunk Documents for RAG?
- **Context Window Limits**: LLMs have a finite amount of text they can consider at once (the context window). Large documents need to be broken down.
- **Precise Retrieval**: Smaller chunks allow the retrieval system to find and provide more specific and relevant pieces of information to the LLM, rather than entire large documents where the relevant detail might be diluted.
- **Semantic Coherence**: Good chunking strategies aim to keep semantically related text together. `RecursiveCharacterTextSplitter` is designed for this by trying to split on sensible separators (like newlines, sentences) first.
- **Embedding Quality**: Embeddings are often more effective when generated for coherent, moderately sized pieces of text.

**Action Required**: Create a directory named `input_docs` in the same location as this notebook and add some sample text files (e.g., `.txt`, `.md`) into it. For example, you could create `doc1.txt` and `doc2.md` with some content.
"""

# Set the input directory path
INPUT_DIR = "./input_docs"  # Ensure this directory exists and contains your documents

# Create directory if it doesn't exist (and inform the user)
if not os.path.exists(INPUT_DIR):
    os.makedirs(INPUT_DIR)
    print(f"Created input directory: {INPUT_DIR}. Please add your documents to this folder.")
elif not os.listdir(INPUT_DIR):
    print(f"Input directory {INPUT_DIR} is empty. Please add your documents to this folder for the RAG pipeline to work.")

#print the count of files in the input directory
file_count = len([f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))])
print(f"Found {file_count} files in the input directory: {INPUT_DIR}")

# Load documents from directory using UnstructuredFileLoader for each file
loader = DirectoryLoader(
    INPUT_DIR,
    glob="**/*.*",  # Loads all files. You can be more specific, e.g., "**/*.txt" or "**/*.md"
    loader_cls=UnstructuredFileLoader,
    show_progress=True,
    use_multithreading=True, # Can speed up loading for many files
    silent_errors=True # Silently skips files that UnstructuredFileLoader can't process
)

documents = []
try:
    print(f"Loading documents from {INPUT_DIR}...")
    documents = loader.load()
    if documents:
        print(f"Successfully loaded {len(documents)} documents.")
        # print(f"First document content (first 100 chars): {documents[0].page_content[:100]}")
        # print(f"First document metadata: {documents[0].metadata}")
    elif os.listdir(INPUT_DIR):
        print(f"Loaded 0 documents. This might be because no files matched the glob or they couldn't be processed. Check file types and `unstructured` dependencies.")
    else:
        print("No documents found in the input directory to load.")
except Exception as e:
    print(f"Error loading documents: {e}")
    documents = [] # Ensure documents is an empty list on error

# Initialize text splitter
# RecursiveCharacterTextSplitter tries to split based on a list of characters (e.g., "\n\n", "\n", " ", "").
# chunk_size: The maximum size of a chunk (in characters, by default).
# chunk_overlap: The number of characters to overlap between chunks to maintain context.
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Max characters per chunk
    chunk_overlap=200,    # Characters of overlap between chunks
    length_function=len   # How to measure chunk size (using len() for characters)
)

# Split documents into chunks
chunks = []
if documents:
    print(f"Splitting {len(documents)} documents into chunks...")
    chunks = text_splitter.split_documents(documents)
    print(f"Successfully created {len(chunks)} chunks from the documents.")
    # if chunks:
        # print(f"First chunk content (first 100 chars): {chunks[0].page_content[:100]}")
        # print(f"First chunk metadata: {chunks[0].metadata}")
else:
    print("No documents were loaded, so no chunks to create.")

"""## 5. Generate and Store Embeddings in Pinecone

Now we'll generate embeddings for our document chunks using the `HuggingFaceEmbeddings` (with `all-MiniLM-L6-v2` model) we initialized earlier. These embeddings will then be stored in our Pinecone index.

### What are Embeddings?
Embeddings are numerical representations (vectors) of text that capture its semantic meaning. Key properties:
- **Text to Vectors**: They convert textual data into a format that machine learning models can understand and process.
- **Semantic Similarity**: Texts with similar meanings will have embeddings that are close together in the vector space (e.g., as measured by cosine similarity).
- **Foundation for Search**: This property allows us to perform semantic search: find documents or chunks that are conceptually similar to a query, not just those that share keywords.

We will use LangChain's Pinecone vector store integration (`LangChainPinecone.from_documents`) which simplifies the process of generating embeddings for our chunks and upserting (uploading/inserting) them into the specified Pinecone index.
"""

# The 'embeddings' model (HuggingFaceEmbeddings) is already initialized in cell 0bcf5a08.

# Store embeddings in Pinecone using LangChain's Pinecone integration
if chunks:
    print(f"Generating embeddings for {len(chunks)} chunks and storing them in Pinecone index '{INDEX_NAME}'...")
    # LangChainPinecone.from_documents handles embedding generation and upsertion to Pinecone
    # It requires the document chunks, the embedding model, and the target index name.
    # This might take some time depending on the number of chunks.
    vectorstore = LangChainPinecone.from_documents(
        documents=chunks,          # The LangChain Document chunks
        embedding=embeddings,      # The initialized HuggingFace embedding function
        index_name=INDEX_NAME      # The name of your Pinecone index
        # pinecone_api_key=PINECONE_API_KEY # Usually picked from environment if not passed
    )
    print(f"Successfully stored embeddings in Pinecone index '{INDEX_NAME}'.")

    # Get updated index statistics from Pinecone directly
    # It might take a few moments for the stats to update after upserting.
    import time
    time.sleep(5) # Give Pinecone a moment to update stats
    stats = pc.Index(INDEX_NAME).describe_index_stats()
    print(f"\nUpdated Index Statistics for '{INDEX_NAME}':")
    print(f"Total vectors: {stats.total_vector_count}")
    print(f"Dimension: {stats.dimension}")
    print(f"Namespaces: {stats.namespaces}")
else:
    print("No chunks available to generate embeddings or store in Pinecone.")
    # If there are no chunks, we still need a vectorstore object for the chat if we want it to run (it just won't find anything)
    # Or, we can initialize from an existing index if it might have data from previous runs.
    try:
        print(f"Attempting to connect to existing index '{INDEX_NAME}' for retriever initialization.")
        vectorstore = LangChainPinecone.from_existing_index(index_name=INDEX_NAME, embedding=embeddings)
        print("Successfully connected to existing index for retriever (if it contained data).")
    except Exception as e:
        print(f"Could not connect to existing index '{INDEX_NAME}' for retriever. Chat functionality might be limited: {e}")
        vectorstore = None # Ensure chat cell can handle this

"""## 6. Create Conversational Retrieval Chain and Chat Interface

Finally, we'll set up a conversational retrieval chain and a simple chat interface. This chain will:
1. Take a user's question.
2. Use the Pinecone vector store (as a retriever) to find the most relevant document chunks based on the question's embedding.
3. Pass the question and the retrieved chunks (as context) to an LLM (e.g., gpt-4.1).
4. The LLM generates a response based on both the question and the provided context from your documents.
5. The chain also manages chat history to allow for follow-up questions.

### How RAG Improves LLM Responses in this Context:
- **Contextual Grounding**: The LLM isn't just relying on its pre-trained knowledge; it's given specific, relevant information from *your* documents to formulate its answer.
- **Reduced Hallucination**: With relevant facts at hand, the LLM is less likely to invent information.
- **Source-Awareness**: The `return_source_documents=True` parameter allows us to see which chunks were used by the LLM, providing transparency and allowing users to consult the original text.
- **Dynamic Knowledge**: If you update the documents in Pinecone, the RAG system will use the new information without needing to retrain the LLM.
"""

# Initialize the Large Language Model (LLM)
# We'll use ChatOpenAI with gpt-4.1, a cost-effective and capable model.
# Temperature=0 makes the output more deterministic and factual.
llm = ChatOpenAI(
    temperature=0,                # Lower temperature for more factual responses
    model="gpt-4.1",        # You can try other models like "gpt-4"
    openai_api_key=OPENAI_API_KEY # Pass the API key
)

# Create the retriever from our Pinecone vector store
# If vectorstore was not successfully initialized (e.g., no chunks), this might cause an error or an ineffective retriever.
if vectorstore:
    retriever = vectorstore.as_retriever(
        search_type="similarity", # Other options: "mmr" (Maximal Marginal Relevance)
        search_kwargs={"k": 3}    # Retrieve top 3 most relevant chunks
    )
    print(f"Retriever created using Pinecone index '{INDEX_NAME}'.")

    # Create Conversational Retrieval Chain
    # This chain combines a retriever with an LLM to answer questions based on retrieved documents
    # and maintain conversation history.
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True, # Set to True to see which documents were retrieved
        verbose=False                 # Set to True for more detailed logging from the chain
    )
    print("Conversational Retrieval Chain created successfully.")
else:
    print("Vectorstore not available. Skipping Conversational Retrieval Chain setup.")
    qa_chain = None

# Simple chat function
chat_history = [] # Initialize an empty chat history

def chat_with_rag_pipeline(query: str):
    global chat_history
    if not qa_chain:
        print("QA chain is not initialized. Cannot process query.")
        return "Error: QA chain not set up."

    try:
        print(f"\nUser Query: {query}")
        # Pass the query and the current chat history to the chain
        result = qa_chain.invoke({"question": query, "chat_history": chat_history})

        answer = result["answer"]
        print(f"LLM Answer: {answer}")

        # Update chat history
        chat_history.append((query, answer))

        print("\nSources Used:")
        if result["source_documents"]:
            for i, doc in enumerate(result["source_documents"]):
                source_name = doc.metadata.get('source', 'Unknown source')
                # Truncate page_content for display
                content_preview = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                print(f"  {i+1}. Source: {source_name}\n     Content Preview: {content_preview}\n")
        else:
            print("  No specific source documents were heavily relied upon or returned by the retriever.")

        return answer
    except Exception as e:
        error_message = f"Error during chat interaction: {str(e)}"
        print(error_message)
        return error_message

# Example Usage (only if documents were processed)
if chunks and qa_chain: # Ensure chunks were processed and chain is ready
    print("\n--- Starting Chat Session ---")
    # First question
    question1 = "What are the main topics covered in the provided documents?"
    response1 = chat_with_rag_pipeline(question1)

    # Follow-up question (demonstrating chat history usage)
    if "Error:" not in response1: # Proceed only if first question was successful
        question2 = "Can you elaborate on one of those topics? Pick one you find interesting."
        response2 = chat_with_rag_pipeline(question2)
else:
    print("\nSkipping chat example as no documents were processed or QA chain is not set up.")
    print("Please add documents to the 'input_docs' folder and re-run the notebook if you wish to chat.")

"""## 7. Conclusion and Next Steps

Congratulations! You've successfully built and tested a Retrieval Augmented Generation (RAG) pipeline using Pinecone and LangChain. This pipeline can:
1. Load and process documents from a local directory.
2. Generate vector embeddings for these documents and store them in a Pinecone serverless index.
3. Retrieve relevant document chunks based on user queries.
4. Use a Large Language Model (LLM) to generate contextual and factual responses based on the retrieved information, while maintaining conversation history.

### Possible Enhancements and Further Exploration:
- **More Document Types**: Extend the `DirectoryLoader` or use specific loaders (e.g., `PyPDFLoader`) to handle PDFs, Word documents, HTML, etc. Remember `unstructured` already supports many.
- **Advanced Chunking Strategies**: Explore different text splitters (e.g., `SemanticChunker`) or chunking parameters for optimal retrieval.
- **Metadata Filtering**: Add metadata to your documents during ingestion (e.g., categories, dates) and modify the retriever to filter based on this metadata for more targeted search.
- **Prompt Engineering**: Customize the prompts used by `ConversationalRetrievalChain` to better guide the LLM's responses and tone. LangChain offers `PromptTemplate` for this.
- **Different Embedding Models**: Experiment with other embedding models from HuggingFace or other providers (e.g., OpenAI embeddings) to see their impact on retrieval quality. Remember to adjust `EMBEDDING_DIMENSION` in Pinecone accordingly.
- **Different LLMs**: Try other LLMs available through LangChain (e.g., GPT-4, models from Hugging Face Hub, Anthropic's Claude).
- **Evaluation**: Implement metrics to evaluate the quality of retrieval and generation (e.g., RAGAs, LangChain's evaluation tools).
- **User Interface**: Build a more sophisticated UI using tools like Streamlit or Gradio.
- **Productionization**: Consider aspects like error handling, logging, scaling, and security for a production deployment.

### Common Use Cases for RAG:
- **Customer Support Chatbots**: Answer customer questions based on product manuals, FAQs, and knowledge bases.
- **Internal Knowledge Base Search**: Help employees quickly find information within internal company documents.
- **Documentation Search and Q&A**: Allow users to ask questions about technical documentation.
- **Research Assistance**: Summarize and answer questions based on a corpus of research papers.
- **Personalized Learning Tools**: Create educational tools that adapt to a student's queries based on learning materials.

### Important: Clean Up Resources
Pinecone indexes incur costs. If you are done with this demo, remember to delete your Pinecone index to avoid ongoing charges.
You can do this via the Pinecone console or programmatically:
```python
# import os
# from pinecone import Pinecone
# from dotenv import load_dotenv
# load_dotenv()
# PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
# pc = Pinecone(api_key=PINECONE_API_KEY)
# INDEX_NAME = "rag-demo-notebook" # Make sure this matches your index name
# if INDEX_NAME in pc.list_indexes().names:
#     print(f"Deleting index '{INDEX_NAME}'...")
#     pc.delete_index(INDEX_NAME)
#     print(f"Index '{INDEX_NAME}' deleted successfully.")
# else:
#     print(f"Index '{INDEX_NAME}' not found.")
```
"""